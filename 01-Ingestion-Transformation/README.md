## AWS Certification Study Guide: Data Ingestion and Transformation

### Domain 1: Data Ingestion and Transformation

#### Task Statement 1.1: Perform Data Ingestion

**Knowledge Areas:**

- Understanding throughput and latency characteristics for AWS services used in data ingestion.
- Exploring data ingestion patterns, including frequency and data history.
- Handling streaming data ingestion and batch data ingestion (scheduled and event-driven).
- Considering the replayability of data ingestion pipelines and managing stateful and stateless data transactions.

**Skills:**

- Reading data from streaming sources such as Amazon Kinesis, Amazon MSK, and AWS Glue.
- Extracting data from batch sources like Amazon S3, AWS Glue, and Amazon EMR.
- Setting up schedulers and event triggers using services like Amazon EventBridge and Apache Airflow.
- Implementing appropriate configurations for batch ingestion and consuming data APIs.
- Managing fan-in and fan-out for streaming data distribution.

#### Task Statement 1.2: Transform and Process Data

**Knowledge Areas:**

- Creating ETL pipelines based on business requirements and understanding data volume, velocity, and variety.
- Utilizing cloud and distributed computing, and leveraging Apache Spark for data processing.
- Identifying intermediate data staging locations for optimization.

**Skills:**

- Optimizing container usage with services like Amazon EKS and Amazon ECS.
- Connecting to different data sources using JDBC and ODBC.
- Integrating data from multiple sources and optimizing costs during data processing.
- Troubleshooting and debugging transformation failures and performance issues.

#### Task Statement 1.3: Orchestrate Data Pipelines

**Knowledge Areas:**

- Integrating various AWS services to create ETL pipelines and understanding event-driven architecture.
- Configuring AWS services for data pipelines based on schedules or dependencies and implementing serverless workflows.

**Skills:**

- Building workflows for data ETL pipelines using orchestration services like Lambda, EventBridge, and AWS Step Functions.
- Ensuring performance, availability, scalability, resiliency, and fault tolerance in data pipelines.
- Using notification services to send alerts.

#### Task Statement 1.4: Apply Programming Concepts

**Knowledge Areas:**

- Implementing continuous integration and continuous delivery (CI/CD) for data pipelines.
- Proficiency in SQL queries, infrastructure as code (IaC), distributed computing, and data structures/algorithms.

**Skills:**

- Optimizing code for data ingestion and transformation, and configuring Lambda functions for concurrency and performance.
- Performing SQL queries for data transformation and optimization.
- Using Git commands for repository management and AWS SAM for deploying serverless data pipelines.
